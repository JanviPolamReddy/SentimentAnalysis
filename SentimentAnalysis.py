# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/176axmvFMIGhBvQuDziGRKgsLkVZ0Irt7
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertModel
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, classification_report
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm

df = pd.read_csv("amazon_reviews.csv")

df = df.drop(columns=['Unnamed: 0'])

df = df.dropna(subset=['reviewText'])

df = df[df['reviewText'].str.strip().astype(bool)]

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

def tokenize_reviews(reviews, tokenizer, max_length):
    input_ids = []
    attention_masks = []

    for review in reviews:
        encoded_dict = tokenizer.encode_plus(
                            review,
                            add_special_tokens = True,
                            max_length = max_length,
                            padding = 'max_length',
                            truncation = True,
                            return_attention_mask = True,
                            return_tensors = 'pt'
                       )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    return input_ids, attention_masks

max_length = 128
train_input_ids, train_attention_masks = tokenize_reviews(train_df['reviewText'], tokenizer, max_length)
test_input_ids, test_attention_masks = tokenize_reviews(test_df['reviewText'], tokenizer, max_length)

class Classifier(nn.Module):
    def __init__(self, bert_model):
        super(Classifier, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(768, 5)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output)
        logits = self.fc(pooled_output)
        return logits

classifier_model = Classifier(bert_model)

batch_size = 32
epochs = 3
learning_rate = 2e-5

train_dataset = TensorDataset(train_input_ids, train_attention_masks, torch.tensor(train_df['overall'].values))
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(test_input_ids, test_attention_masks, torch.tensor(test_df['overall'].values))
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(classifier_model.parameters(), lr=learning_rate)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*epochs)

for epoch in range(epochs):
    classifier_model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc="Epoch {}".format(epoch + 1)):
        input_ids, attention_mask, labels = batch
        optimizer.zero_grad()
        logits = classifier_model(input_ids, attention_mask)

        labels = labels - 1

        labels = labels.long()

        loss = criterion(logits, labels)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
        scheduler.step()


    average_loss = total_loss / len(train_loader)
    print("Epoch {} - Average Loss: {}".format(epoch + 1, average_loss))

classifier_model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Evaluation"):
        input_ids, attention_mask, labels = batch
        logits = classifier_model(input_ids, attention_mask)
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_preds)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(all_labels, all_preds))